{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmir3nmtMkW3wvsyfnjpqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanchanraiii/SecureRag/blob/master/Faiss_gemini_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xASSCr_Cwe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9204b80e-5c56-4312-aeec-99ecf6225d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m41.0/49.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \"langchain[google-genai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai scikit-learn spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "print(\"✅ All libraries are installed and the API key is set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC2x2I3dDkt8",
        "outputId": "1bbec2c4-85f1-4dff-ebc9-faf1664fce01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/9.5 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m7.3/9.5 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Enter API key for Google Gemini: ··········\n",
            "✅ All libraries are installed and the API key is set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M0ePfUTFFXtj",
        "outputId": "90e38cf3-2037-4a7d-987d-8b8d1c57b700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "kWk-VMzrEIkO",
        "outputId": "62cced83-89f8-4ec0-baa4-2059c94d18f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8aa47f8a-98f7-4509-a63e-928db9c1872c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8aa47f8a-98f7-4509-a63e-928db9c1872c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving finance_dataset.jsonl to finance_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "file_path = \"finance_dataset.jsonl\"\n",
        "chunks = []\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        document_text = f.read()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_text(document_text)\n",
        "\n",
        "    print(f\"✅ Successfully loaded and chunked '{file_path}'. Found {len(chunks)} chunks.\")\n",
        "    if chunks:\n",
        "        print(\"\\n--- Sample Chunk 1 ---\")\n",
        "        print(chunks[0])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
        "    print(\"Please make sure you have uploaded the file and the name is correct.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_cdaKAWbOx8",
        "outputId": "95293b9b-19e4-4bd8-d7de-1cb3163d729d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully loaded and chunked 'finance_dataset.jsonl'. Found 9045 chunks.\n",
            "\n",
            "--- Sample Chunk 1 ---\n",
            "{\"name\": \"Tejas BhatQagar\", \"account_number\": \"HYZH9588211631790\", \"balance\": 869258.08, \"loan_application\": {\"amount\": 375634.85, \"status\": \"approved\", \"application_date\": \"2023-06-25\"}, \"transaction_history\": [{\"date\": \"2025-01-06\", \"amount\": 49243.76, \"type\": \"debit\", \"description\": \"orchestrate strategic models\"}, {\"date\": \"2025-07-01\", \"amount\": 49243.76, \"type\": \"credit\", \"description\": \"architect compelling infrastructures\"}, {\"date\": \"2025-05-12\", \"amount\": 49243.76, \"type\": \"debit\", \"description\": \"integrate rich eyeballs\"}], \"email\": \"yashasvi62@example.net\", \"phone\": \"09577266631\", \"pan\": \"BMZMR13953\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGPipeline:\n",
        "    def __init__(self, embedding_model_name: str, llm_name: str):\n",
        "        print(\"--- Initializing Models and Filters ---\")\n",
        "        self.embedding_model = GoogleGenerativeAIEmbeddings(model=embedding_model_name)\n",
        "        self.llm = ChatGoogleGenerativeAI(model=llm_name)\n",
        "        self.vector_store = {}\n",
        "\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.REGEX_PATTERNS = {\n",
        "            \"EMAIL\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n",
        "            \"PHONE\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
        "            \"CREDIT_CARD\": r\"\\b(?:\\d[ -]*?){13,16}\\b\",\n",
        "            \"SSN\": r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",\n",
        "        }\n",
        "        print(\"--- Models and Filters Initialized ---\")\n",
        "\n",
        "    def _input_filter(self, query: str, threshold: float = 0.5) -> (bool, str):\n",
        "        \"\"\"Filters the user's query for sensitive information.\"\"\"\n",
        "        doc = self.nlp(query)\n",
        "        pii_count = sum(1 for ent in doc.ents if ent.label_ in [\"PERSON\", \"GPE\", \"LOC\", \"ORG\"])\n",
        "        pii_count += sum(len(re.findall(pattern, query)) for pattern in self.REGEX_PATTERNS.values())\n",
        "\n",
        "        if len(query.split()) > 0 and pii_count / len(query.split()) > threshold:\n",
        "            return True, \"Query blocked due to high concentration of sensitive information.\"\n",
        "        return False, \"Query is safe.\"\n",
        "\n",
        "    def _output_filter(self, response: str) -> str:\n",
        "        \"\"\"Redacts sensitive information from the RAG model's output.\"\"\"\n",
        "        doc = self.nlp(response)\n",
        "        redacted_text = list(response)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in [\"PERSON\", \"GPE\", \"LOC\", \"ORG\"]:\n",
        "                start, end = ent.start_char, ent.end_char\n",
        "                redacted_text[start:end] = f\"[{ent.label_}]\"\n",
        "        redacted_text = \"\".join(redacted_text)\n",
        "        for pii_type, pattern in self.REGEX_PATTERNS.items():\n",
        "            redacted_text = re.sub(pattern, f\"[{pii_type}]\", redacted_text)\n",
        "        return redacted_text\n",
        "\n",
        "    def build_vector_store(self, text_chunks: list, batch_size: int = 100):\n",
        "        \"\"\"Builds the vector store by embedding chunks in smaller, trackable batches.\"\"\"\n",
        "        total_chunks = len(text_chunks)\n",
        "        print(f\"\\n--- Building Vector Store for {total_chunks} chunks ---\")\n",
        "        for i in range(0, total_chunks, batch_size):\n",
        "            batch_chunks = text_chunks[i:i + batch_size]\n",
        "            batch_embeddings = self.embedding_model.embed_documents(batch_chunks)\n",
        "            for j, (chunk, embedding) in enumerate(zip(batch_chunks, batch_embeddings)):\n",
        "                self.vector_store[i + j] = {\"text\": chunk, \"embedding\": np.array(embedding).reshape(1, -1)}\n",
        "            print(f\"Processed {min(i + batch_size, total_chunks)} / {total_chunks} chunks...\")\n",
        "        print(\"--- Vector Store Created ---\\n\")\n",
        "\n",
        "    def _get_single_embedding(self, text: str) -> np.ndarray:\n",
        "        embedding = self.embedding_model.embed_query(text)\n",
        "        return np.array(embedding).reshape(1, -1)\n",
        "\n",
        "    def run_query(self, query: str):\n",
        "        print(f\"Processing query: '{query}'\")\n",
        "        print(\"=\"*30)\n",
        "\n",
        "        is_sensitive, message = self._input_filter(query)\n",
        "        if is_sensitive:\n",
        "            print(f\"Input Filter Action: {message}\")\n",
        "            return\n",
        "\n",
        "        query_embedding = self._get_single_embedding(query)\n",
        "\n",
        "        similarities = []\n",
        "        for i, data in self.vector_store.items():\n",
        "            sim = cosine_similarity(query_embedding, data[\"embedding\"])[0][0]\n",
        "            similarities.append((sim, data[\"text\"]))\n",
        "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
        "        context = [text for sim, text in similarities[:2]]\n",
        "\n",
        "        print(\"\\n--- Retrieved Context ---\")\n",
        "        for c in context:\n",
        "            print(f\"- {c[:150]}...\")\n",
        "        print(\"-\" * 25, \"\\n\")\n",
        "\n",
        "        context_str = \"\\n\".join(context)\n",
        "        prompt = f\"Context:\\n{context_str}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
        "        result = self.llm.invoke(prompt)\n",
        "\n",
        "        print(f\"\\nOriginal Generated Response: '{result.content}'\")\n",
        "        safe_response = self._output_filter(result.content)\n",
        "        print(f\"Final (Redacted) Response: '{safe_response}'\\n\")\n",
        "\n",
        "print(\"✅ RAG Pipeline class is defined with integrated filters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEoavIbsbpbL",
        "outputId": "988e748b-97bc-434a-ad66-9f41f0960ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RAG Pipeline class is defined with integrated filters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'chunks' in locals() and chunks:\n",
        "    pipeline = RAGPipeline(\n",
        "        embedding_model_name=\"models/text-embedding-004\",\n",
        "        llm_name=\"gemini-1.5-flash\"\n",
        "    )\n",
        "\n",
        "    pipeline.build_vector_store(chunks)\n",
        "\n",
        "    test_query = \"Who is Teerth Badal?\"\n",
        "\n",
        "    # --- THIS IS THE CORRECTED LINE ---\n",
        "    pipeline.run_query(test_query)\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot run the pipeline because no chunks were loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow8ywAQBbs5z",
        "outputId": "c32aaeea-4787-4f9e-9091-49209f569abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Models and Filters ---\n",
            "--- Models and Filters Initialized ---\n",
            "\n",
            "--- Building Vector Store for 9045 chunks ---\n",
            "Processed 100 / 9045 chunks...\n",
            "Processed 200 / 9045 chunks...\n",
            "Processed 300 / 9045 chunks...\n",
            "Processed 400 / 9045 chunks...\n",
            "Processed 500 / 9045 chunks...\n",
            "Processed 600 / 9045 chunks...\n",
            "Processed 700 / 9045 chunks...\n",
            "Processed 800 / 9045 chunks...\n",
            "Processed 900 / 9045 chunks...\n",
            "Processed 1000 / 9045 chunks...\n",
            "Processed 1100 / 9045 chunks...\n",
            "Processed 1200 / 9045 chunks...\n",
            "Processed 1300 / 9045 chunks...\n",
            "Processed 1400 / 9045 chunks...\n",
            "Processed 1500 / 9045 chunks...\n",
            "Processed 1600 / 9045 chunks...\n",
            "Processed 1700 / 9045 chunks...\n",
            "Processed 1800 / 9045 chunks...\n",
            "Processed 1900 / 9045 chunks...\n",
            "Processed 2000 / 9045 chunks...\n",
            "Processed 2100 / 9045 chunks...\n",
            "Processed 2200 / 9045 chunks...\n",
            "Processed 2300 / 9045 chunks...\n",
            "Processed 2400 / 9045 chunks...\n",
            "Processed 2500 / 9045 chunks...\n",
            "Processed 2600 / 9045 chunks...\n",
            "Processed 2700 / 9045 chunks...\n",
            "Processed 2800 / 9045 chunks...\n",
            "Processed 2900 / 9045 chunks...\n",
            "Processed 3000 / 9045 chunks...\n",
            "Processed 3100 / 9045 chunks...\n",
            "Processed 3200 / 9045 chunks...\n",
            "Processed 3300 / 9045 chunks...\n",
            "Processed 3400 / 9045 chunks...\n",
            "Processed 3500 / 9045 chunks...\n",
            "Processed 3600 / 9045 chunks...\n",
            "Processed 3700 / 9045 chunks...\n",
            "Processed 3800 / 9045 chunks...\n",
            "Processed 3900 / 9045 chunks...\n",
            "Processed 4000 / 9045 chunks...\n",
            "Processed 4100 / 9045 chunks...\n",
            "Processed 4200 / 9045 chunks...\n",
            "Processed 4300 / 9045 chunks...\n",
            "Processed 4400 / 9045 chunks...\n",
            "Processed 4500 / 9045 chunks...\n",
            "Processed 4600 / 9045 chunks...\n",
            "Processed 4700 / 9045 chunks...\n",
            "Processed 4800 / 9045 chunks...\n",
            "Processed 4900 / 9045 chunks...\n",
            "Processed 5000 / 9045 chunks...\n",
            "Processed 5100 / 9045 chunks...\n",
            "Processed 5200 / 9045 chunks...\n",
            "Processed 5300 / 9045 chunks...\n",
            "Processed 5400 / 9045 chunks...\n",
            "Processed 5500 / 9045 chunks...\n",
            "Processed 5600 / 9045 chunks...\n",
            "Processed 5700 / 9045 chunks...\n",
            "Processed 5800 / 9045 chunks...\n",
            "Processed 5900 / 9045 chunks...\n",
            "Processed 6000 / 9045 chunks...\n",
            "Processed 6100 / 9045 chunks...\n",
            "Processed 6200 / 9045 chunks...\n",
            "Processed 6300 / 9045 chunks...\n",
            "Processed 6400 / 9045 chunks...\n",
            "Processed 6500 / 9045 chunks...\n",
            "Processed 6600 / 9045 chunks...\n",
            "Processed 6700 / 9045 chunks...\n",
            "Processed 6800 / 9045 chunks...\n",
            "Processed 6900 / 9045 chunks...\n",
            "Processed 7000 / 9045 chunks...\n",
            "Processed 7100 / 9045 chunks...\n",
            "Processed 7200 / 9045 chunks...\n",
            "Processed 7300 / 9045 chunks...\n",
            "Processed 7400 / 9045 chunks...\n",
            "Processed 7500 / 9045 chunks...\n",
            "Processed 7600 / 9045 chunks...\n",
            "Processed 7700 / 9045 chunks...\n",
            "Processed 7800 / 9045 chunks...\n",
            "Processed 7900 / 9045 chunks...\n",
            "Processed 8000 / 9045 chunks...\n",
            "Processed 8100 / 9045 chunks...\n",
            "Processed 8200 / 9045 chunks...\n",
            "Processed 8300 / 9045 chunks...\n",
            "Processed 8400 / 9045 chunks...\n",
            "Processed 8500 / 9045 chunks...\n",
            "Processed 8600 / 9045 chunks...\n",
            "Processed 8700 / 9045 chunks...\n",
            "Processed 8800 / 9045 chunks...\n",
            "Processed 8900 / 9045 chunks...\n",
            "Processed 9000 / 9045 chunks...\n",
            "Processed 9045 / 9045 chunks...\n",
            "--- Vector Store Created ---\n",
            "\n",
            "Processing query: 'Who is Teerth Badal?'\n",
            "==============================\n",
            "\n",
            "--- Retrieved Context ---\n",
            "- {\"name\": \"Teerth Badal\", \"account_number\": \"GESQ8997297564522\", \"balance\": 569901.59, \"loan_application\": {\"amount\": 194903.94, \"status\": \"pending\", \"...\n",
            "- {\"name\": \"Teerth Bal\", \"account_number\": \"BKVN3193060218864\", \"balance\": 463596.55, \"loan_application\": {\"amount\": 308870.33, \"status\": \"approved\", \"a...\n",
            "------------------------- \n",
            "\n",
            "\n",
            "Original Generated Response: 'Teerth Badal is an individual with account number GESQ8997297564522, a balance of 569901.59, and a pending loan application for 194903.94 submitted on 2024-05-24.  Their email is banerjeeharsh@examOle.net and their phone number is +912390506192. Their PAN is TAVUS98729.'\n",
            "Final (Redacted) Response: '[PERSON] is an individual with account number GESQ[PHONE]522, a balance of 569901.59, and a pending loan application for 194903.94 submitted on 2024-05-24.  Their email is [EMAIL] and their phone number is +[PHONE]92. Their PAN [ORG]TAVUS98729.'\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
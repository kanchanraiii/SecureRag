{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanchanraiii/SecureRag/blob/master/Faiss_gemini_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xASSCr_Cwe9"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \"langchain[google-genai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required packages\n",
        "!pip install -qU langchain-google-genai scikit-learn spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import getpass\n",
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "\n",
        "# Set up your Google API Key\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "print(\"✅ All libraries are installed and the API key is set.\")"
      ],
      "metadata": {
        "id": "WC2x2I3dDkt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "M0ePfUTFFXtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "kWk-VMzrEIkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary text splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Define the file path for your uploaded document\n",
        "file_path = \"finance_dataset.jsonl\"\n",
        "chunks = []\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        document_text = f.read()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_text(document_text)\n",
        "\n",
        "    print(f\"✅ Successfully loaded and chunked '{file_path}'. Found {len(chunks)} chunks.\")\n",
        "    if chunks:\n",
        "        print(\"\\n--- Sample Chunk 1 ---\")\n",
        "        print(chunks[0])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
        "    print(\"Please make sure you have uploaded the file and the name is correct.\")"
      ],
      "metadata": {
        "id": "A_cdaKAWbOx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGPipeline:\n",
        "    def __init__(self, embedding_model_name: str, llm_name: str):\n",
        "        print(\"--- Initializing Models and Filters ---\")\n",
        "        self.embedding_model = GoogleGenerativeAIEmbeddings(model=embedding_model_name)\n",
        "        self.llm = ChatGoogleGenerativeAI(model=llm_name)\n",
        "        self.vector_store = {}\n",
        "\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        # Regex for PII detection\n",
        "        self.REGEX_PATTERNS = {\n",
        "            \"EMAIL\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n",
        "            \"PHONE\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
        "            \"CREDIT_CARD\": r\"\\b(?:\\d[ -]*?){13,16}\\b\",\n",
        "            \"SSN\": r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",\n",
        "        }\n",
        "        print(\"--- Models and Filters Initialized ---\")\n",
        "\n",
        "    def _input_filter(self, query: str, threshold: float = 0.5) -> (bool, str):\n",
        "        \"\"\"Filters the user's query for sensitive information.\"\"\"\n",
        "        doc = self.nlp(query)\n",
        "        pii_count = sum(1 for ent in doc.ents if ent.label_ in [\"PERSON\", \"GPE\", \"LOC\", \"ORG\"])\n",
        "        pii_count += sum(len(re.findall(pattern, query)) for pattern in self.REGEX_PATTERNS.values())\n",
        "\n",
        "        if len(query.split()) > 0 and pii_count / len(query.split()) > threshold:\n",
        "            return True, \"Query blocked due to high concentration of sensitive information.\"\n",
        "        return False, \"Query is safe.\"\n",
        "\n",
        "    def _output_filter(self, response: str) -> str:\n",
        "        \"\"\"Redacts sensitive information from the RAG model's output.\"\"\"\n",
        "        doc = self.nlp(response)\n",
        "        redacted_text = list(response)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in [\"PERSON\", \"GPE\", \"LOC\", \"ORG\"]:\n",
        "                start, end = ent.start_char, ent.end_char\n",
        "                redacted_text[start:end] = f\"[{ent.label_}]\"\n",
        "        redacted_text = \"\".join(redacted_text)\n",
        "        for pii_type, pattern in self.REGEX_PATTERNS.items():\n",
        "            redacted_text = re.sub(pattern, f\"[{pii_type}]\", redacted_text)\n",
        "        return redacted_text\n",
        "\n",
        "    def build_vector_store(self, text_chunks: list, batch_size: int = 100):\n",
        "        \"\"\"Builds the vector store by embedding chunks in smaller, trackable batches.\"\"\"\n",
        "        total_chunks = len(text_chunks)\n",
        "        print(f\"\\n--- Building Vector Store for {total_chunks} chunks ---\")\n",
        "        for i in range(0, total_chunks, batch_size):\n",
        "            batch_chunks = text_chunks[i:i + batch_size]\n",
        "            batch_embeddings = self.embedding_model.embed_documents(batch_chunks)\n",
        "            for j, (chunk, embedding) in enumerate(zip(batch_chunks, batch_embeddings)):\n",
        "                self.vector_store[i + j] = {\"text\": chunk, \"embedding\": np.array(embedding).reshape(1, -1)}\n",
        "            print(f\"Processed {min(i + batch_size, total_chunks)} / {total_chunks} chunks...\")\n",
        "        print(\"--- Vector Store Created ---\\n\")\n",
        "\n",
        "    def _get_single_embedding(self, text: str) -> np.ndarray:\n",
        "        embedding = self.embedding_model.embed_query(text)\n",
        "        return np.array(embedding).reshape(1, -1)\n",
        "\n",
        "    def run_query(self, query: str):\n",
        "        print(f\"Processing query: '{query}'\")\n",
        "        print(\"=\"*30)\n",
        "\n",
        "        # Call the internal input filter\n",
        "        is_sensitive, message = self._input_filter(query)\n",
        "        if is_sensitive:\n",
        "            print(f\"Input Filter Action: {message}\")\n",
        "            return\n",
        "\n",
        "        query_embedding = self._get_single_embedding(query)\n",
        "\n",
        "        similarities = []\n",
        "        for i, data in self.vector_store.items():\n",
        "            sim = cosine_similarity(query_embedding, data[\"embedding\"])[0][0]\n",
        "            similarities.append((sim, data[\"text\"]))\n",
        "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
        "        context = [text for sim, text in similarities[:2]]\n",
        "\n",
        "        print(\"\\n--- Retrieved Context ---\")\n",
        "        for c in context:\n",
        "            print(f\"- {c[:150]}...\")\n",
        "        print(\"-\" * 25, \"\\n\")\n",
        "\n",
        "        context_str = \"\\n\".join(context)\n",
        "        prompt = f\"Context:\\n{context_str}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
        "        result = self.llm.invoke(prompt)\n",
        "\n",
        "        print(f\"\\nOriginal Generated Response: '{result.content}'\")\n",
        "        # Call the internal output filter\n",
        "        safe_response = self._output_filter(result.content)\n",
        "        print(f\"Final (Redacted) Response: '{safe_response}'\\n\")\n",
        "\n",
        "print(\"✅ RAG Pipeline class is defined with integrated filters.\")"
      ],
      "metadata": {
        "id": "AEoavIbsbpbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure chunks were loaded successfully in Cell 3\n",
        "if 'chunks' in locals() and chunks:\n",
        "    # 1. Initialize the pipeline\n",
        "    pipeline = RAGPipeline(\n",
        "        embedding_model_name=\"models/text-embedding-004\",\n",
        "        llm_name=\"gemini-1.5-flash\"\n",
        "    )\n",
        "\n",
        "    # 2. Build the vector store using the chunks from Cell 3\n",
        "    pipeline.build_vector_store(chunks)\n",
        "\n",
        "    # 3. Run a test query\n",
        "    # Replace this with any query relevant to your document\n",
        "    test_query = \"Who is Teerth Badal?\"\n",
        "\n",
        "    # --- THIS IS THE CORRECTED LINE ---\n",
        "    pipeline.run_query(test_query)\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot run the pipeline because no chunks were loaded.\")"
      ],
      "metadata": {
        "id": "ow8ywAQBbs5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}